\documentclass[10pt,showpacs,preprintnumbers,footinbib,amsmath,amssymb,aps,prl,twocolumn,groupedaddress,superscriptaddress,showkeys]{revtex4-1}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{color}
\usepackage{amsmath}
\usepackage{multirow}

\newcommand{\costa}[1]{% \costa{<power>}
	\ensuremath{\cos ^{#1} {\theta}} }
\newcommand{\sinta}[1]{% \sinta{<power>}
	\ensuremath{\sin ^{#1} {\theta}} }
\newcommand{\pwrten}[1]{%\pwrten{<power>}
	\ensuremath{10^{#1}} }
\newcommand{\rhomax}{
	\ensuremath{ \rho _{\mathrm{max}}} }

\begin{document}
\title{Project 2}
\author{Thomas Redpath}
\affiliation{Department of Physics, Michigan State University, Outer Space}
\begin{abstract}
We present our Ferrari algorithm for solving linear equations. Our best algorithm runs as $4n$ FLOPS with $n$ the dimensionality of the matrix.
\end{abstract}
\maketitle

\section{Introduction}

In this project we solve Schr\"{o}edinger's equation for two electrons
in a spherically symmetric 3D harmonic oscillator (HO) potential first
without then including the Coulomb interaction. We proceed by
discretizing the radial Schr\"{o}edinger equation to cast it as an
eigenvalue problem. We then employ Jacobi's method to diagonalize
and obtain the eigenvalues and eigenvectors.

\section{Theory}

First, consider the radial part of Schr\"{o}edinger's equation for a
single electron with a HO potential given by $V(r) = 1/2 kr^2$
where $k = m \omega^2$

\begin{equation*}
  -\frac{\hbar^2}{2 m} \left ( \frac{1}{r^2} \frac{d}{dr} r^2
  \frac{d}{dr} - \frac{l (l + 1)}{r^2} \right )R(r) 
     + V(r) R(r) = E R(r).
\end{equation*}
The energy eigenvalues are given by

\begin{equation*}
E_{nl}=  \hbar \omega \left(2n+l+\frac{3}{2}\right),
\end{equation*}
with $n=0,1,2,\dots$ and $l=0,1,2,\dots$, where $n$ and $l$
are the principle angular momentum quantum numbers
respectively.

With the substitution $R(r) = (1/r) u(r)$, the dimensionless variable
$\rho = (1/ \alpha) r$ and taking $l=0$ we obtain

\begin{equation*}
  -\frac{d^2}{d\rho^2} u(\rho) 
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
\end{equation*}
where $\alpha$ is a constant with dimension length.

Fixing $\alpha$ such that

\begin{equation*}
\frac{mk}{\hbar^2} \alpha^4 = 1 \Rightarrow \alpha = \left(\frac{\hbar^2}{mk}\right)^{1/4}
\end{equation*}
and defining

\begin{equation*}
\lambda = \frac{2m\alpha^2}{\hbar^2}E,
\end{equation*}
we can rewrite Schr\"{o}edinger's equation as

\begin{equation}
  -\frac{d^2}{d\rho^2} u(\rho) + \rho^2u(\rho)  = \lambda u(\rho) .
\label{eq:1bseq}
\end{equation}
In 3D with $l=0$, the eigenvalues are
$\lambda_0=3,\lambda_1=7,\lambda_2=11,\dots .$

It is now straightforward to see how this equation can be solved numerically using the standard
numerical second derivative

\begin{equation}
    u''=\frac{u(\rho+h) -2u(\rho) +u(\rho-h)}{h^2} +O(h^2),
    \label{eq:diffoperation}
\end{equation}
with $N$ grid points and the step size $h = (\rho _N - \rho _0) / N$ where $\rho_0, \rho_N$
are the maximum and minimum values chosen for the grid. The value of $\rho$ at the
$i^{\mathrm{th}$ grid point is then $\rho _i = \rho _0 + ih; i = 1,2,\dots,N$.

In the compact ``discretized notation,'' Sch\"{o}dinger's equation becomes

\begin{equation*}
-\frac{u_{i+1} -2u_i +u_{i-1}}{h^2}+\rho_i^2u_i = \lambda u_i.
\end{equation*}
Representing the diagonal matrix elements as

\begin{equation*}
   d_i=\frac{2}{h^2}+V_i,
\end{equation*}
and the non-diagonal matrix element
\begin{equation*}
   e_i=-\frac{1}{h^2}.
\end{equation*}
we arrive at a matrix eigenvalue equation representation of the problem

\begin{equation*}
    \begin{bmatrix}d_0 & e_0 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & d_1 & e_1 & 0    & \dots  &0     &0 \\
                                0   & e_2 & d_2 & e_2  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots  e_{N-1}     &d_{N-1} & e_{N-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{N} & d_{N}
             \end{bmatrix}  \begin{bmatrix} u_{0} \\
                                                              u_{1} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{N}
             \end{bmatrix}=\lambda \begin{bmatrix} u_{0} \\
                                                              u_{1} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{N}
             \end{bmatrix}.  
%      \label{eq:sematrix}
\end{equation*}

To account for the Coulomb interaction between two electons, it is convenient to
introduce a change of coordinates (eq.~\ref{eq:changecoordinates}) that allows
us to separates the Schr\"{o}dinger
equation into two parts, one that describes the center of mass ($R$) of the two-body
system and one that describes the interaction of the two electrons ($r$).

\begin{align}\label{eq:changecoordinates}
\begin{split}
	r &= r_1 - r_2\\
	R &= \frac{1}{2} r_1 + r_2
\end{split}
\end{align}
With this change of coordinates, we may re-write the two-body Schr\"{o}dinger
equation

\begin{align*}
\left(  -\frac{\hbar^2}{2 m} \frac{d^2}{dr_1^2} -\frac{\hbar^2}{2 m} \frac{d^2}{dr_2^2}+ \frac{1}{2}k r_1^2+ \frac{1}{2}k r_2^2\right)u(r_1,r_2) \\
 = E^{(2)} u(r_1,r_2)
\end{align*}
as

\begin{align*}
\left(  -\frac{\hbar^2}{m} \frac{d^2}{dr^2} -\frac{\hbar^2}{4 m} \frac{d^2}{dR^2}+ \frac{1}{4} k r^2+  kR^2\right)u(r,R)\\
= E^{(2)} u(r,R)
\end{align*}
(see Appendix) and the ansatz for the wavefunction $\Psi(r,R) = \psi(r) \phi(R)$
allows us to separate the wavefunction such that the two-body eigenenergies
are a sum of the center of mass and interaction energies $E ^{(2)} = E _R + E_r$.
With the form of the Coulomb interaction between two electrons

\begin{equation*}
V(r_1,r_2) = \frac{\beta e^2}{|\mathbf{r}_1-\mathbf{r}_2|}=\frac{\beta e^2}{r},
\end{equation*}
 $\beta e^2=1.44$ eVnm, and the dimensionless variable $\rho = r/\alpha$ the
Schr\"{o}dinger equation becomes

\begin{equation*}
  -\frac{d^2}{d\rho^2} \psi(\rho) 
       + \frac{1}{4}\frac{mk}{\hbar^2} \alpha^4\rho^2\psi(\rho)+\frac{m\alpha \beta e^2}{\rho\hbar^2}\psi(\rho)  = 
\frac{m\alpha^2}{\hbar^2}E_r \psi(\rho) .
\end{equation*}
By defining an ``effective'' oscillator frequency

\begin{equation*}
\omega_r^2=\frac{1}{4}\frac{mk}{\hbar^2} \alpha^4,
\end{equation*}
fixing $\alpha$ such that

\begin{equation*}
\frac{m\alpha \beta e^2}{\hbar^2}=1 \Rightarrow \alpha = \frac{\hbar^2}{m\beta e^2}
\end{equation*}
and letting

\begin{equation*}
\lambda = \frac{m\alpha^2}{\hbar^2}E,
\end{equation*}
the Schr\"{o}dinger equation becomes

\begin{equation*}
  -\frac{d^2}{d\rho^2} \psi(\rho) + \omega_r^2\rho^2\psi(\rho) +\frac{1}{\rho} = \lambda \psi(\rho).
\end{equation*}
which has a similar form to eq.~\ref{eq:1bseq}. The same discretization treatment
described above can be applied to this equation to arrive at the same matrix
eigenvalue equation as before, just with a different potential. Namely, the
$V_i$ term in the diagonal matrix elements becomes $\omega_r ^2 \rho + 1/\rho$
instead of $\rho ^2$.



\subsection{Unitary transformations preserve orthogonality}

Consider a set of vectors $\mathbf{v}_i$ that form an orthonormal basis,
\[
\mathbf{v}_i = \begin{bmatrix} v_{i1} \\ \dots \\ \dots \\v_{in} \end{bmatrix}
\]
We assume that the basis is orthogonal, that is 
\[
\mathbf{v}_j^T\mathbf{v}_i = \delta_{ij}.
\]
Now consider a set of vectors $\mathbf{w}_i$ derived from applying an orthogonal
transformation to the original vectors $\mathbf{v}_i$
\[
\mathbf{w}_i=\mathbf{S}\mathbf{v}_i,
\]
The dot product between two transformed vectors is preserved due to the
property $\mathbf{S}^T \mathbf{S} = \mathbf{I}$ of orthogonal matrices
or $\mathbf{U} ^\dagger \mathbf{U} = \mathbf{I}$ of unitary matrices.
So, the transformed vectors are orthogonal like the original vectors.
\begin{align*}
\mathbf{w}_j^T\mathbf{w}_i &= (\mathbf{S} \mathbf{v}_j)^T (\mathbf{S}\mathbf{v}_i)\\
	&= \mathbf{v}_j ^T \mathbf{S}^T \mathbf{S} \mathbf{v}_i\\
	&= \mathbf{v}_j ^T \mathbf{v}_i\\
	&= \delta_{ij}.
\end{align*}

\section{Algorithms and methods}

To solve the matrix eigenvalue problem, we applied Jacobi's method within a
C++ program to diagonalize the $N \times N$ matrix. The source code developed
for this project is available at \url{https://github.com/redpath11/phy905_thr}.
The directory \texttt{projects/project2/} inside that repository contains the
source code described below. All file paths given here are relative to that
directory of the git repository.

\subsection{Jacobi's Method}

%It should be
%noted that this method is not the most efficient for solving this problem, especially
%since the matrix is already tridiagonal. It would be more computationally efficient to
%use 

Jacobi's method involves applying a series of similarity transformations
to bring the original symmetric tridiagonal matrix to a diagonal one. At the end
of this process, we obtain the eigenvalues and eigenvectors. The similarity
transformation can be interpreted geometrically as a rotation
about an angle $\theta$ in the n-dimensional Euclidean space. We denote
such a transformation of the matrix $\mathbf{A}$ into the matrix
$\mathbf{B}$ as

\begin{equation*}
	\mathbf{B} = \mathbf{S} ^T \mathbf{A} \mathbf{S}.
\end{equation*}
where $\mathbf{S}$ has the form

\begin{equation*}
	\mathbf{S} = 
	\begin{bmatrix}1 & 0 & \dots   & 0    & 0  & \dots  & 0 & 0 \\
                                0 & 1 & \dots & 0    & 0  & \dots     &0 & 0 \\
			\dots & \dots & \dots & \dots & \dots & \dots & \dots & \dots\\
                                0 & 0 & \dots & \costa{}  & 0   & \dots & 0 & \sinta{} \\
			0 & 0 & \dots & 0 & 1 & \dots & 0 & 0 \\
			\dots & \dots & \dots & \dots & \dots & \dots & \dots & \dots\\
                                0   & 0 & \dots & 0  & 0  \dots  & 1 & 0\\
                                0   & 0 & \dots & - \sinta{}  &\dots & \dots & 0 & \costa{}
	\end{bmatrix}
	.
\end{equation*}
The resulting matrix elements are

\begin{align*}
	b _{ii}  &= a _{ii}, i \neq k, i \neq l\\
	b _{ik} &= a _{ik} \cos{\theta} - a _{il} \sin{\theta}, i \neq k, i \neq l\\
	b _{il}  &= a _{il} \cos{\theta} + a _{ik} \sin{\theta}, i \neq k, i \neq l\\
	b _{kk} &= a _{kk} \costa{2} - 2 a_{kl} \costa{} \sinta{} + a _{ll} \sinta{2}\\
	b _{ll}  &= a _{ll} \costa{2} + 2 a _{al} \costa{} \sinta{} + a _{kk} \sinta{2}\\
	b _{kl} &= (a _{kk} - a _{ll} ) \costa{} \sinta{} + a _{kl} (\costa{2} - \sinta{2} )
\end{align*}
where $\theta$ for each transformation is chosen such that the off diagonal $b _{kl}$
is zero. We re-write this condition with the abbreviations $t = \tan{\theta}$ and the
definition

\begin{equation*}
	\tau = \frac{a _{ll} - a _{kk}}{2 a _{kl}}
\end{equation*}
as

\begin{equation*}
	t ^2  + 2 \tau t - 1 = 0
\end{equation*}
which has solutions

\begin{equation*}
	t = - \tau \pm \sqrt{1 + \tau ^2}.
\end{equation*}
Now, from $\costa{2} + \sinta{2} = 1$ we can write

\begin{equation*}
	\costa{} = \frac{1}{ \sqrt{1 + t ^2}}
\end{equation*}
from which we can get $\sinta{} = t \costa{}$.

With this procedure for zeroing an off-diagonal matrix element, we can generate
a recipe for iteratively diagonalizing any symmetric matrix:

\begin{enumerate}
	\item Set some numerical limit that determines when the matrix elements
	are small enough to be effectively zero (e.g. \pwrten{-8})
	\item Find the largest matrix element
	\item Compute $\tau,t,\costa{},\sinta{}$ that will zero this matrix element
	\item Apply the similarity transformation defined in the previous step to the
	matrix to obtain a transformed matrix
	\item Repeat the previous three steps until all matrix elements are less than
	the limit set in the first step
\end{enumerate}

\subsection{Implementing Jacobi's method}

We implemented Jacobi's method in a C++ function that performs the steps listed
in the previous section. This function sets the limit from step one in the previous
section to \pwrten{-8} as well as a limit on the maximum number of iterations ($n^3$,
where $n$ is the dimensionality of the matrix) in case the off-diagonal limit
isn't satisfied. We then, loop until one of these conditions is met. For each iteration,
the function \texttt{FindMaxOffDiag} searches the current version of the matrix for
the largest off diagonal matrix and \texttt{Rotate} computes $\theta$ then applies
similarity transformation. The similarity transformation of each iteration is also applied
to a second $N \times N$ matrix that was initialized to the identity. This allows us to
obtain the eigenvectors. After the loop is complete, the original tridiagonal matrix
has been transformed into a diagonal matrix with the eigenvalues along the main
diagonal. One final loop over the main diagonal is performed to find the lowest
eigenvalue which is printed out along with its index. We use the index to extract
the corresponding eigenvector from the eigenvector matrix.


 
\subsection{Unit Tests}
During development of our code, we implemented a series of unit tests to verify that
we were coding the algorithm correctly. The first test was to ensure that we setup
the tridiagonal matrix correctly. The function \texttt{SetMatrix} generates a matrix
of the specified dimension and \rhomax. We wrote this matrix out to a file
\texttt{Benchmark/test0.out} that was used as input to a Python script that employs
the \texttt{numpy.linalg.eigvals} solver to find the eigenvalues. In this way, we verified
that the non-interacting matrix was being properly set up by checking that the lowest
eigenvalues were approximately the analytic HO eigenvalues (3,7,11,15,...). This test
was implemented via the \texttt{test0} function in \texttt{src/proj2.cc}.

The second test checked that the search for the largest off-diagonal matrix element
works properly. In this simple test, we set up a $4 \times 4$ matrix and initialize a
few of the off-diagonal elements to random values. We then run the \texttt{FindMaxOffDiag}
function on this matrix and ensure that it returns the proper element. Furthermore, we
checked that the larget off-diagonal element was chosen based on the absolute value
of the element. This test was coded in the \texttt{test1} function in \texttt{src/proj2.cc}.

Our third unit test checked the Jacobi solver results against the output from 
\texttt{numpy.linalg.eigvals} for simple $3 \times 3$ and $4 \times 4$ matrices stored in the
files \texttt{Benchmark/test2\_3.in} and \texttt{Benchmark/test2\_4.in} respectively. The
function \texttt{test2} in \texttt{src/proj2.cc} reads in one of these matrices, applys the
Jacobi algorithm and checks that the eigenvectors are orthogonal.

The next test we performed checked that our code calculates eigenfunctions that make
sense physically. We ran the Jacobi algorithm with 50 gridpoints and $\rhomax=5$ to
check the behavior the first three eigenfuctions - see FIG.~\ref{fig:NIwvfn}.

\begin{figure}
\centering
	\includegraphics[width=0.5\textwidth]{figures/NIwvfn.pdf}
	\caption{The square modulus of the ground state (blue) and first two excited state (red and
	green) wavefunctions with computed with $N=50, \rhomax=5$.}
	\label{fig:NIwvfn}
\end{figure}


\section{Results and discussions}

\subsection{The non-interacting case}

We tested how many grid points and what values for \rhomax give
the correct results for the lowest three eigenvalues to four decimal places.
The results are summarized in Table~\ref{tab:NRhoResults}. A
$\rhomax = 5$ appears to be large enough to give correct predictions
for the first three eigenvalues. This can also be seen in FIG.~\ref{fig:NIwvfn}
where all three wavefunctions go to zero before \rhomax is reached.
In order to attain the desired four-decimal
place precision, we increased the number of grid points to 500. In general,
both \rhomax and the number of grid points contribute to the accuracy of
the numerical eigenvalues since together they determine the how close
the calculation approaches the continuous case (how fine-grained the grid is).
To accurately reproduce the higher
eigenenergies, we need a larger \rhomax to ensure that the boundary
condition $u(\rhomax)=0$ is satisfied for the more extended
wavefunctions. If we only care about the the
lowest eigenenergies, we can keep \rhomax small for the same number
of grid points and achieve a better approximation to the continuous
case.

In TABLE~\ref{tab:w} we list the simulation parameters ($N,\rhomax$)
and the ground state eigenenergy for different oscillator strengths ($\omega$).
In general, the smaller ($\omega$) the shallower the oscillator potential which
shifts the eigenenergies lower. This also makes the wavefunctions more diffuse
and requires a larger \rhomax to ensure that the upper boundary condition is
satisfied. We varied $N$ and \rhomax until the ground state energy converged.
We kept $N$ at or below 250 grid points to limit the run time of the program.
TABLE~\ref{tab:w} shows that for the shallowest oscillator potential ($\omega = 0.01$)
a large \rhomax is needed to capture the ground state wavefunction. For the
other three cases, $\rhomax=5$ was sufficient. The ground state wavefunctions
with different $\omega$ are plotted in FIG.~\ref{fig:gsw}. Note that the extent of
the ground state wavefunction increases with decreasing $\omega$.

%% Compare Jacobi solver to python implementation run times %%

\begin{table}
\centering
	\begin{tabular}{ c c | c c c | c }
	 & & \multicolumn{3}{c}{$2 E_i = 4i + 3$} &\\
\hline
%	 &  & $2E_1$ & $2E_2$ & $2E_3$ &\\
	 & & 3 & 7 & 11 & \\
\hline
	\multicolumn{6}{c}{Numerical Results}\\
\hline
	$N$ & \rhomax & $2E_1$ & $2E_2$ & $2E_3$ & \# iterations\\
\hline
	20  & 5  & 2.980329 & 6.900902 & 10.752631 & 609\\
	50  & 5  & 2.996871 & 6.984340 & 10.961832 & 4047\\
	100 & 5 & 2.999219 & 6.996093 & 10.990595 & 16532\\
	200 & 5 & 2.999805 & 6.999025 & 10.997780 & 66618\\
	500 & 5 & 2.999969 & 6.999846 & 10.999802 & 421903\\
\hline
	50   & 10 & 2.987443 & 6.936917 & 10.845289 & 3681\\
	100 & 10 & 2.996871 & 6.984339 & 10.961741 & 15522\\
	200 & 10 & 2.999219 & 6.996092 & 10.990460 & 64026\\
\hline
	50   & 15 & 2.971582 & 6.856332 & 10.645120 & 2718\\
	100 & 15 & 2.992951 & 6.964661 & 10.913531 & 14312\\
	200 & 15 & 2.998241 & 6.991200 & 10.978512 & 61378\\
	\end{tabular}
	\caption{Lowest three eigenvalues for different values of
	$N$ and \rhomax. The first two lines give the analytic
	formula and the corresponding values for the first three
	eigenenergies of the harmonic oscillator potential.}
	\label{tab:NRhoResults}
\end{table}

\begin{figure}
\centering
	\includegraphics[width=0.5\textwidth]{figures/dnit.pdf}
	\caption{The number of similarity transformations need to
	diagonalized the matrix is plotted against the matrix
	dimensionality. Note the general $N^2$ dependence of
	the number of iterations.}
	\label{fig:dnit}
\end{figure}

\begin{table}
\centering
	\begin{tabular}{ c | c c c }
	$\omega$ & $N$ & \rhomax & $E_0$\\
\hline
	\multirow{6}{*}{0.01} & 100 & 10 & 0.0996\\
	        & 150 & 15 & 0.0496\\
	        & 200 & 20 & 0.0351\\
	        & 250 & 25 & 0.0399\\
	        & 250 & 50 & 0.0299\\
	        & 250 & 100&0.0299\\
\hline
	\multirow{4}{*}{0.25} & 100 & 10 & 1.4992\\
				     & 150 & 15 & 1.4992\\
				     &  50  & 5   & 1.4994\\
				     & 250 & 5   & 1.5002\\
\hline
	1.0 & 100 & 10 & 2.9969\\
\hline
	\multirow{3}{*}{5.0} & 100 & 10 & 14.9214\\
	      & 100 & 5   & 14.9884\\
	      & 250 & 5   & 14.9969
	\end{tabular}
	\caption{Ground state eigenenergies for different combinations of
	($N,\rhomax$) and different oscillator strengths ($\omega$) to check
	for convergence of $E_0$ for the different $\omega$ (non-interacting case).}
	\label{tab:w}
\end{table}

\begin{figure}
\centering
	\includegraphics[width=0.5\textwidth]{figures/gsw.pdf}
	\caption{Ground state wavefunctions for different oscillator
	strengths. The weakest potential ($\omega = 0.01$) is shown in black and the
	strongest potential ($\omega = 5$) is plotted in green.
	The wavefunctions have been scaled to the $\omega = 5$ wavefunction.}
	\label{fig:gsw}
\end{figure}
%\begin{figure}[hbtp]
%\includegraphics[scale=0.4]{test1.pdf}
%\caption{Exact and numerial solutions for $n=10$ mesh points.} 
%\label{fig:n10points}
%\end{figure}

\section{Conclusions}

\begin{thebibliography}{99}
%%\bibitem{miller2006} G.~A.~Miller, A.~K.~Opper, and E.~J.~Stephenson, Annu.~Rev.~Nucl.~Sci.~{\bf 56}, 253 (2006).
\end{thebibliography}

\end{document}